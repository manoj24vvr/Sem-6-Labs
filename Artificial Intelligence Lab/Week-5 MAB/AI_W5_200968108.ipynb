{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Week-5:** MULTI ARMED BANDITS IN TF-AGENTS\n",
        "### **Name:** Atyam V V R Manoj\n",
        "### **Reg No. | Sec :** 200968108 | DSE-A 27"
      ],
      "metadata": {
        "id": "6dOaEAjpu7rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing & importing required libraries"
      ],
      "metadata": {
        "id": "ucM_ZYZnvT5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-JLkfrpzpXH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10eefca-b2db-4e0f-c28f-7851e5fdc425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.8/dist-packages (0.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "from tf_agents.bandits.agents import lin_ucb_agent\n",
        "from tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\n",
        "from tf_agents.bandits.metrics import tf_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "nest = tf.nest"
      ],
      "metadata": {
        "id": "J3mUTO-MxCvr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Bandit problem, the observations are independent, so to abstract away thye concept of RL episodes, we define two subclasses of *PyEnvironment* and *TFEnvironment*: **BanditPyEnvironment** and **BanditTFEnvironment.**"
      ],
      "metadata": {
        "id": "rnuw4BW9yIE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a BanditPyEnvironment class\n",
        "\n",
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  # Helper functions.\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                 self.observation_spec())\n",
        "\n",
        "  # These two functions below should not be overridden by subclasses.\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns a time step containing an observation.\"\"\"\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  def _step(self, action):\n",
        "    \"\"\"Returns a time step containing the reward for the action taken.\"\"\"\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  # These two functions below are to be implemented in subclasses.\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    \"\"\"Returns an observation.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "3-uBmaC3x6FE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 1 -** Create a  environment\n",
        "\n",
        "**a. for  which  the  observation  is  a  random  integer  between -5  and  5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.**"
      ],
      "metadata": {
        "id": "S7-dqs-QvYBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a SimplePyEnvironment with above mentioned param\n",
        "\n",
        "class SimplePyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec( shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation\n",
        "\n",
        "\n",
        "#  We can use this environment to get the observations, and receive rewards for our actions\n"
      ],
      "metadata": {
        "id": "XKx_clmBv7_G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the above environment\n",
        "\n",
        "environment = SimplePyEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(\"observation: %d\" % observation)\n",
        "\n",
        "action = 1\n",
        "\n",
        "print(\"action: %d\" % action)\n",
        "reward = environment.step(action).reward\n",
        "print(\"reward: %f\" % reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51XoexZPz6CQ",
        "outputId": "25514306-8d37-4ccf-b03c-5cd663a9520b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation: 2\n",
            "action: 1\n",
            "reward: 2.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define a Bandit environment by wraping the BanditPyEnvironment subclass with *TFPyEnvironment.*"
      ],
      "metadata": {
        "id": "Hc_91Y49009e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ],
      "metadata": {
        "id": "tPKpqJbe0xc5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.**"
      ],
      "metadata": {
        "id": "BLoIEgFUwZCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "5jbMj4QmwZLp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can request an observation from the environment, call the policy to choose an action, then the environment will output the reward."
      ],
      "metadata": {
        "id": "3xRnohw82GUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Request  for  50  observations  from  the  environment,  compute  and  print  the total reward.**"
      ],
      "metadata": {
        "id": "0BkMsWUfwcpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sign_policy = SignPolicy()\n",
        "all_rewards = []\n",
        "\n",
        "# defining a 'for' loop for running the 50 observations\n",
        "for i in range(1,51):\n",
        "  print(\"Observation \",i,\":\")\n",
        "  current_time_step = tf_environment.reset()\n",
        "  print('Observation:')\n",
        "  print (current_time_step.observation)\n",
        "  action = sign_policy.action(current_time_step).action\n",
        "  print('Action:')\n",
        "  print (action)\n",
        "  reward = tf_environment.step(action).reward\n",
        "  print('Reward:')\n",
        "  print(reward)\n",
        "  all_rewards.append(reward)\n",
        "  print(\"\\n\")\n",
        "\n",
        "print(\"Total Rewards: \", sum(all_rewards))\n",
        "\n"
      ],
      "metadata": {
        "id": "3wRuc4IAwcuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83abd33a-5a7a-48ce-80bd-c6f9522bf385"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation  1 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  2 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  3 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  4 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  5 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  6 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  7 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  8 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  9 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  10 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  11 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  12 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  13 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  14 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  15 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  16 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  17 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  18 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  19 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  20 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  21 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  22 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  23 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  24 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  25 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  26 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  27 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  28 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  29 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  30 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  31 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  32 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  33 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  34 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  35 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  36 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  37 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  38 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  39 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  40 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  41 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  42 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  43 :\n",
            "Observation:\n",
            "tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  44 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  45 :\n",
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  46 :\n",
            "Observation:\n",
            "tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  47 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  48 :\n",
            "Observation:\n",
            "tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  49 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Observation  50 :\n",
            "Observation:\n",
            "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "\n",
            "\n",
            "Total Rewards:  tf.Tensor([[66.]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see total rewards as **66** in the above case"
      ],
      "metadata": {
        "id": "ypXYlXQ2I148"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 2** – Create an environment\n",
        "\n",
        "**a. Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.**"
      ],
      "metadata": {
        "id": "4hTtT8UAwB5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "\n",
        "    # Flipping the sign with probability 1/2.\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n",
        "\n",
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
      ],
      "metadata": {
        "id": "YHl30wP7wUI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe935ab-0078-4854-b4e8-893fe6858cae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward sign:\n",
            "-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:**\n",
        "\n",
        "1. The agent has not detected know yet which version of the environment is running.\n",
        "\n",
        "2. The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
        "\n",
        "3. The  agent  detected  that  the  flipped  version  of  the  environment  is running."
      ],
      "metadata": {
        "id": "tCNbqDv8wjd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a tf_variable named _situation to store this information encoded as values in [0, 2], then make the policy behave accordingly."
      ],
      "metadata": {
        "id": "_3BxAhxr4F_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      # Choose 1 so that we get information on the sign.\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    # Choose 0 or 2, depending on the situation and the sign of the observation.\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "hH6N1zdmwjjw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c.Define the agent that detects the sign of the environment and sets the policy appropriately.**"
      ],
      "metadata": {
        "id": "VZcowvTYw9dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "\n",
        "    # We only need to change the value of the situation variable if it is\n",
        "    # unknown (0) right now, and we can infer the situation only if the\n",
        "    # observation is not 0.\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      \"\"\"This returns either 1 or 2, depending on the signs.\"\"\"\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())\n",
        "\n",
        "sign_agent = SignAgent()"
      ],
      "metadata": {
        "id": "PSTQxcnuw9hT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion:**\n",
        "* All the above defined environments and policies can now be used to further train and implement a MAB agent with specific ance metrics such as the *Regret metric.*\n",
        "\n",
        "(**Regret Metric:** difference between the reward collected by the agent and the expected reward of an oracle policy that has access to the reward functions of the environment.)\n"
      ],
      "metadata": {
        "id": "QT3ZT1eawnH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_optimal_reward(observation):\n",
        "  expected_reward_for_arms = [\n",
        "      tf.linalg.matvec(observation, tf.cast(arm0_param, dtype=tf.float32)),\n",
        "      tf.linalg.matvec(observation, tf.cast(arm1_param, dtype=tf.float32)),\n",
        "      tf.linalg.matvec(observation, tf.cast(arm2_param, dtype=tf.float32))]\n",
        "  optimal_action_reward = tf.reduce_max(expected_reward_for_arms, axis=0)\n",
        "  return optimal_action_reward\n",
        "\n",
        "regret_metric = tf_metrics.RegretMetric(compute_optimal_reward)"
      ],
      "metadata": {
        "id": "G8rW2sb5G7-U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here, we are implementing the StationaryStochasticPyEnvironment. This environment takes as parameter a (usually noisy) function for giving observations (context), and for every arm takes an (also noisy) function that computes the reward based on the given observation.\n",
        "* The reward functions are linear functions of the context, plus some Gaussian noise."
      ],
      "metadata": {
        "id": "DLLbST3LHIBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2 # @param\n",
        "arm0_param = [-3, 0, 1, -2] # @param\n",
        "arm1_param = [1, -2, 3, 0] # @param\n",
        "arm2_param = [0, 0, 1, 1] # @param\n",
        "def context_sampling_fn(batch_size):\n",
        "  \"\"\"Contexts from [-10, 10]^4.\"\"\"\n",
        "  def _context_sampling_fn():\n",
        "    return np.random.randint(-10, 10, [batch_size, 4]).astype(np.float32)\n",
        "  return _context_sampling_fn\n",
        "\n",
        "class LinearNormalReward(object):\n",
        "  \"\"\"A class that acts as linear reward function when called.\"\"\"\n",
        "  def __init__(self, theta, sigma):\n",
        "    self.theta = theta\n",
        "    self.sigma = sigma\n",
        "  def __call__(self, x):\n",
        "    mu = np.dot(x, self.theta)\n",
        "    return np.random.normal(mu, self.sigma)\n",
        "\n",
        "arm0_reward_fn = LinearNormalReward(arm0_param, 1)\n",
        "arm1_reward_fn = LinearNormalReward(arm1_param, 1)\n",
        "arm2_reward_fn = LinearNormalReward(arm2_param, 1)\n",
        "\n",
        "environment = tf_py_environment.TFPyEnvironment(\n",
        "    sspe.StationaryStochasticPyEnvironment(\n",
        "        context_sampling_fn(batch_size),\n",
        "        [arm0_reward_fn, arm1_reward_fn, arm2_reward_fn],\n",
        "        batch_size=batch_size))\n",
        "\n"
      ],
      "metadata": {
        "id": "Y47bOrXO5a7_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent here is using the *LinUCB* algorithm\n"
      ],
      "metadata": {
        "id": "huEKBrw7Id3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation_spec = tensor_spec.TensorSpec([4], tf.float32)\n",
        "time_step_spec = ts.time_step_spec(observation_spec)\n",
        "action_spec = tensor_spec.BoundedTensorSpec(\n",
        "    dtype=tf.int32, shape=(), minimum=0, maximum=2)\n",
        "\n",
        "agent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)"
      ],
      "metadata": {
        "id": "_465Aooe7DVU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_iterations = 90 # @param\n",
        "steps_per_loop = 1 # @param\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.policy.trajectory_spec,\n",
        "    batch_size=batch_size,\n",
        "    max_length=steps_per_loop)\n",
        "\n",
        "observers = [replay_buffer.add_batch, regret_metric]\n",
        "\n",
        "driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env=environment,\n",
        "    policy=agent.collect_policy,\n",
        "    num_steps=steps_per_loop * batch_size,\n",
        "    observers=observers)\n",
        "\n",
        "regret_values = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  driver.run()\n",
        "  loss_info = agent.train(replay_buffer.gather_all())\n",
        "  replay_buffer.clear()\n",
        "  regret_values.append(regret_metric.result())\n",
        "\n",
        "plt.plot(regret_values)\n",
        "plt.ylabel('Average Regret')\n",
        "plt.xlabel('Number of Iterations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "0EZ4pQf_7DTH",
        "outputId": "9a5cf060-abae-4032-f338-ff1815f8bd67"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-49404e5a0f70>:23: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Number of Iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observation:**\n",
        "From the above resuting plot, we can see that it shows average Regret is decreasing as the agent is trained further. And the policy gets better in learning the right action to be taken for a given observation as input."
      ],
      "metadata": {
        "id": "dagzwjhUHu4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbpkoIWSIN9r"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}