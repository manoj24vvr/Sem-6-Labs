{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEp06kZGlq1"
      },
      "source": [
        "### **Week-8:** MDP and Dynamic Programming\n",
        "### **Name:** Atyam V V R Manoj\n",
        "### **Reg No. | Sec:** 200968108"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmFXdrKQC2WU"
      },
      "source": [
        "### Use the Frozen Lake environment.\n",
        "\n",
        "https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
        "#### **Importing required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KvGIxp4eyALi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJSJF7YHB-v3"
      },
      "source": [
        "#### Intializing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8HXEZGtyIQh",
        "outputId": "adef981c-6f61-40d8-b4ef-6619166542ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1',is_slippery=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraTBWVvCEZo"
      },
      "source": [
        "#### Defining a **Helper function to calculate a state-value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QAoTteifyI2c"
      },
      "outputs": [],
      "source": [
        "# Calculate a state-value function\n",
        "def one_step_lookahead(env, state, V, discount):\n",
        "    '''\n",
        "    V: 2-D tensor\n",
        "        Matrix of size nSxnA, each cell represents \n",
        "        a probability of taking actions\n",
        "    '''\n",
        "    \n",
        "    n_actions = env.action_space.n\n",
        "    action_values = np.zeros(shape=n_actions)\n",
        "    for action in range(n_actions):\n",
        "        for prob, next_state, reward, done in env.env.P[state][action]:\n",
        "            action_values[action] += prob * (reward + discount * V[next_state])\n",
        "    ''' \n",
        "    return:\n",
        "        A vector of length nA containing the expected value of each action\n",
        "    '''\n",
        "    return action_values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDbW5lK4CrXI"
      },
      "source": [
        "#### Defining a function to **implement an optimal policy** for the Frozen Lake environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EheuMWhxyJd0"
      },
      "outputs": [],
      "source": [
        "def policy_eval(policy, env, discount=1.0, theta=1e-9, max_iter=1000):\n",
        "    \"\"\"    \n",
        "    policy: 2-D tensor\n",
        "        Matrix of size nSxnA, each cell represents \n",
        "        a probability of taking actions\n",
        "    \"\"\"\n",
        "    n_states = env.observation_space.n\n",
        "    eval_iters = 1\n",
        "\n",
        "    # value function\n",
        "    V = np.zeros(shape=n_states)\n",
        "\n",
        "    # repeat until value change is below the threshold\n",
        "    for i in range(int(max_iter)):\n",
        "        delta = 0\n",
        "        for state in range(n_states):\n",
        "            # init a new value of current state\n",
        "            v = 0\n",
        "            # Try all possible actions which can be taken from this state\n",
        "            for action, action_prob in enumerate(policy[state]):\n",
        "                for state_prob, next_state, reward, done in env.P[state][action]:\n",
        "                    # calculate the expected value\n",
        "                    v += action_prob * state_prob * (reward + discount * V[next_state])\n",
        "            # calculate the absolute change of value function\n",
        "            delta = max(delta, np.abs(V[state] - v))\n",
        "            # update value function\n",
        "            V[state] = v\n",
        "        eval_iters += 1\n",
        "        \n",
        "        if delta < theta:\n",
        "            print(f'Policy evaluation terminated after {eval_iters} iterations.\\n')\n",
        "            return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KeF-r8iDcB3"
      },
      "source": [
        "### **1.Create a Policy Iteration function with the following parameters**\n",
        "* policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.\n",
        "* environment: Initialized OpenAI gym environment object\n",
        "* discount_factor: MDP discount factor.\n",
        "* theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "* max_iterations: Maximum number of iterations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IryDdEYmDs8Y"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env, discount=1.0, max_iter=1000):\n",
        "\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    \n",
        "    # start with random policy = n_states x n_actions / n_actions\n",
        "    policy = np.ones(shape=[n_states, n_actions]) / n_actions\n",
        "\n",
        "    # counter of evaluated policies\n",
        "    evaluated_policies = 1\n",
        "    \n",
        "    # repeat until convergence or critical number of iterations reached\n",
        "    for i in range(int(max_iter)):\n",
        "        stable_policy = False\n",
        "\n",
        "        # Evaluate current policy\n",
        "        V = policy_eval(policy, env, discount)\n",
        "        \n",
        "        # go through each state & try to improve actions that were taken\n",
        "        for state in range(n_states):\n",
        "            curr_action = np.argmax(policy[state])\n",
        "            # look one step ahead and evaluate if curr_action is optimal\n",
        "            # will try every possible action in a curr_state\n",
        "            action_value = one_step_lookahead(env, state, V, discount)\n",
        "            # select best aciton \n",
        "            best_action = np.argmax(action_value)\n",
        "            # if action didn't change\n",
        "            if curr_action != best_action:\n",
        "                stable_policy = True\n",
        "            # Greedy policy update\n",
        "            policy[state] = np.eye(n_actions)[best_action]\n",
        "        evaluated_policies += 1\n",
        "        # if the algo converged & policy is not changing anymore\n",
        "        if stable_policy:\n",
        "            print(f'Found stable policy after {evaluated_policies:,} evaluations.\\n')\n",
        "            return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6YgpeNsEJhv"
      },
      "source": [
        "### **2.Create a Value Iteration function with the following parameters**\n",
        "* environment: Initialized OpenAI gym environment object\n",
        "* discount_factor: MDP discount factor\n",
        "* theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this numberd.\n",
        "* max_iterations: Maximum number of iterations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v2-KjjYmyKSM"
      },
      "outputs": [],
      "source": [
        "# defining Value iteration algorithm to solve MDP.\n",
        "\n",
        "def value_iteration(env, discount=1e-1, theta=1e-9, max_iter=1e4):\n",
        "\n",
        "    # initalized state-value function with zeros for each env state\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    \n",
        "    for i in range(int(max_iter)):\n",
        "        # early stopping condition\n",
        "        delta = 0\n",
        "\n",
        "        for state in range(env.observation_space.n):\n",
        "\n",
        "            # Do a one-step lookahead to calculate state-action values\n",
        "            action_value = one_step_lookahead(env, state, V, discount)\n",
        "\n",
        "            # select best action to perform based on the highest state-action values\n",
        "            best_action_value = np.max(action_value)\n",
        "          \n",
        "            # calculate change in value\n",
        "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "            \n",
        "            # update the value function for current state\n",
        "            V[state] = best_action_value\n",
        "            \n",
        "        # checking the condition to exit the loop\n",
        "\n",
        "        if delta < theta:\n",
        "            print(f'\\nValue iteration converged at iteration #{i+1:,}')\n",
        "            break\n",
        "    \n",
        "    policy = np.zeros(shape=[env.observation_space.n, env.action_space.n])\n",
        "    \n",
        "    for state in range(env.observation_space.n):\n",
        "        # one step lookahead to find the best action for this state\n",
        "        action_value = one_step_lookahead(env, state, V, discount)\n",
        "        #select the best action based on the highest state-action value\n",
        "        best_action = np.argmax(action_value)\n",
        "        # update the policy to perform a better action at a current state\n",
        "        policy[state, best_action] = 1.0\n",
        "    \n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_r4EthIF2NY"
      },
      "source": [
        "### Defining a function to implement eacdh episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HA0k2Qh8yK7R"
      },
      "outputs": [],
      "source": [
        "def play_episodes(env, episodes, policy, max_action=100, render=False):\n",
        "\n",
        "    wins = 0\n",
        "    total_reward, total_action = 0, 0\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done, max_a = False, 0 # max_a are the no. of actions taken\n",
        "        while max_a < max_action:\n",
        "            action = np.argmax(policy[state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            if render:\n",
        "                env.render()\n",
        "            max_a += 1\n",
        "            total_reward += reward  # increment reward received\n",
        "            state = next_state  # set current state to next state\n",
        "\n",
        "            # terminate if we're done and increment `wins`\n",
        "            if done and reward == 1:\n",
        "                wins += 1\n",
        "                break\n",
        "        \n",
        "        total_action += max_a\n",
        "\n",
        "    print(f'Total rewards: {total_reward:,}\\tMax action: {max_a:,}')\n",
        "    \n",
        "    avg_reward = total_reward / episodes\n",
        "    avg_action = total_action / episodes\n",
        "    print('')\n",
        "    '''\n",
        "    return: tuple(wins, total_reward, average_reward)\n",
        "        - wins: The total number of wins the agent has\n",
        "        - total_reward: The agent's total accumulated reward\n",
        "        - average_reward: The agent's average reward\n",
        "    '''\n",
        "    return wins, total_reward, avg_reward, avg_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVN5jRuOFO0X"
      },
      "source": [
        "### Implementing the **agent** in the given environment for 1000 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oo2MuWYQFnAW"
      },
      "outputs": [],
      "source": [
        "episodes = 1000\n",
        "\n",
        "def agent(env):\n",
        "\n",
        "    rewards = []\n",
        "  \n",
        "    action_mapping = {\n",
        "        0: '\\u2191',  # up\n",
        "        1: '\\u2192',  # right\n",
        "        2: '\\u2193',  # down\n",
        "        3: '\\u2190'   # left\n",
        "    }\n",
        "    \n",
        "    policies = [\n",
        "        ('Policy Iteration', policy_iteration),\n",
        "        ('Value Iteration', value_iteration)\n",
        "    ]\n",
        "    \n",
        "    for iter_name, iter_func in policies:\n",
        "        policy, V = iter_func(env)\n",
        "        \n",
        "        print(f'Final policy using {iter_name}:')\n",
        "        print(' '.join([action_mapping[action] for action in np.argmax(policy, axis=1)]))\n",
        "        \n",
        "        wins, total_reward, avg_reward, avg_action = play_episodes(env, episodes, policy)\n",
        "        rewards.append(total_reward)\n",
        "        \n",
        "        print(f'number of wins = {wins:,}')\n",
        "        print(f'average reward = {avg_reward:.2f}')\n",
        "        print(f'average action = {avg_action:.2f}')\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2UG0B9OGdjA",
        "outputId": "24c8c327-d839-4220-c689-5f35711e325c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy evaluation terminated after 66 iterations.\n",
            "\n",
            "Found stable policy after 2 evaluations.\n",
            "\n",
            "Final policy using Policy Iteration:\n",
            "↑ ← ↑ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n",
            "Total rewards: 742.0\tMax action: 19\n",
            "\n",
            "number of wins = 742\n",
            "average reward = 0.74\n",
            "average action = 52.82\n",
            "\n",
            "Value iteration converged at iteration #8\n",
            "Final policy using Value Iteration:\n",
            "→ ← ↓ ← ↑ ↑ ↑ ↑ ← → ↑ ↑ ↑ ↓ → ↑\n",
            "Total rewards: 444.0\tMax action: 18\n",
            "\n",
            "number of wins = 444\n",
            "average reward = 0.44\n",
            "average action = 68.73\n"
          ]
        }
      ],
      "source": [
        "rewards = agent(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5MWWXWTgGl6P"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAkTl1D-RPxF",
        "outputId": "3918cc42-e1fe-4142-afda-7fd011569cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[742.0, 444.0]\n"
          ]
        }
      ],
      "source": [
        "print(rewards)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nKPTuOOHEg0A"
      },
      "source": [
        "### **3.Compare  the number of  wins, average  return  after  1000  episodes and  comment  on which method performed better.**\n",
        "1. Based on the above output, we can say the ***Policy Iteration performs better.***\n",
        "2. No. of wins (no. of rewards) is always higher while using the Policy Iteration method(742) than the Value Iteration method(444) and so is the average return.\n",
        "* In **policy iteration**, we start by choosing an random policy ,then we iteratively evaluate and improve the policy until convergence. While in **value iteration**, we start by computing a random state value function and iteratively update the estimate value.\n",
        "* Both are guaranteed to converge but policy iteration algorithm is faster, requires lesser iterations and cheaper to compute.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehWrpDv4qnhb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
